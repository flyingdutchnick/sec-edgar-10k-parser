{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import requests\n",
    "import json\n",
    "import time\n",
    "import logging\n",
    "from datetime import datetime\n",
    "from bs4 import BeautifulSoup\n",
    "from typing import List, Dict, Union\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "REQUEST_DELAY_SECONDS = 0.1  # Delay to ensure we don't exceed 10 requests per second as per SEC policies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the ciks if provided with a list of tickers\n",
    "def get_ciks(tickers_or_ciks: List[str]) -> Union[set, str]:\n",
    "    # initialize the list of ciks and incorrect input values\n",
    "    ciks = []\n",
    "    incorrect_input_values = []\n",
    "\n",
    "    # check if any input value is a ticker\n",
    "    has_tickers = any(re.fullmatch(r'\\b[a-z]{1,5}\\b', input_value.lower()) for input_value in tickers_or_ciks)\n",
    "    \n",
    "    # download ticker.txt and read it into a dictionary if there are any tickers\n",
    "    ticker_cik_dict = {}\n",
    "    if has_tickers:\n",
    "        response = requests.get('https://www.sec.gov/include/ticker.txt')\n",
    "        lines = response.text.split('\\n')\n",
    "        for line in lines:\n",
    "            # ensure line is not empty before splitting\n",
    "            if line.strip():\n",
    "                (key, val) = line.split('\\t')\n",
    "                ticker_cik_dict[key] = val\n",
    "    \n",
    "    # iterate over each input\n",
    "    for input_value in tickers_or_ciks:\n",
    "        # convert input to lower case for comparison\n",
    "        input_value_lower = input_value.lower()\n",
    "        # check if input_value is a ticker or a CIK\n",
    "        if re.fullmatch(r'\\d{4,10}', input_value_lower):  # CIKs are 4-10 digit numbers\n",
    "            ciks.append(input_value_lower)\n",
    "        elif re.fullmatch(r'\\b[a-z]{1,5}\\b', input_value_lower):  # tickers are 1-5 lowercase letters\n",
    "            # look up the corresponding cik in the ticker_cik_dict\n",
    "            if input_value_lower in ticker_cik_dict:\n",
    "                ciks.append(ticker_cik_dict[input_value_lower])\n",
    "            else:\n",
    "                logging.info(f\"CIK not found for ticker {input_value}\")\n",
    "\n",
    "    # return the set of ciks to avoid duplicates\n",
    "    return set(ciks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the filings given a list of ciks over a certain date range\n",
    "def get_filings(cik_numbers: List[str], filing_date_start: str, filing_date_end: str, email_as_user_agent: str, form_type: str = None) -> Dict[str, Dict]:\n",
    "    filings = {}\n",
    "    \n",
    "    for cik_number in cik_numbers:\n",
    "        try:\n",
    "            url = f\"https://data.sec.gov/submissions/CIK{cik_number.zfill(10)}.json\"\n",
    "            headers = {\n",
    "                'User-Agent': email_as_user_agent\n",
    "            }\n",
    "            response = requests.get(url, headers=headers)\n",
    "            \n",
    "            if response.status_code == 200:\n",
    "                original_dict = json.loads(response.text)['filings']['recent']\n",
    "                \n",
    "                # Date range\n",
    "                start_date = datetime.strptime(filing_date_start, '%Y-%m-%d')\n",
    "                end_date = datetime.strptime(filing_date_end, '%Y-%m-%d')\n",
    "                \n",
    "                # Indices of filings that meet the date and form type criteria\n",
    "                indices = [\n",
    "                    i for i, date_str in enumerate(original_dict['filingDate'])\n",
    "                    if start_date <= datetime.strptime(date_str, '%Y-%m-%d') <= end_date and (form_type is None or original_dict['form'][i] == form_type)\n",
    "                ]\n",
    "                \n",
    "                # New dictionary, where each key's value is a list of entries that meet the criteria\n",
    "                new_dict = {\n",
    "                    key: [original_dict[key][i] for i in indices]\n",
    "                    for key in original_dict.keys()\n",
    "                }\n",
    "                \n",
    "                # If no filings are found for a specific date range and form type, log this information\n",
    "                if not new_dict['form']:\n",
    "                    logging.info(f\"No filings found for CIK {cik_number} from {filing_date_start} to {filing_date_end} for form type {form_type}\")\n",
    "                else:\n",
    "                    filings[cik_number] = new_dict\n",
    "        \n",
    "        except requests.RequestException as e:\n",
    "            logging.error(f\"Error occurred while making a request: {str(e)}\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Unexpected error occurred: {str(e)}\")\n",
    "        \n",
    "        time.sleep(REQUEST_DELAY_SECONDS)\n",
    "    \n",
    "    return filings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the starting position of the readable portion of the 10K document\n",
    "def extract_starting_position(input_html):\n",
    "    # Write the regex\n",
    "    regex = re.compile(r'(>Part(\\s|&#160;|&nbsp;)(I)(?=<))|(PART\\s(I)(?=<))')\n",
    "\n",
    "    # Use finditer to match the regex\n",
    "    matches = regex.finditer(input_html)\n",
    "\n",
    "    # Look backwards to find the starting position of the match in the enclosing div\n",
    "    start_positions = []\n",
    "    for match in matches:\n",
    "        match_text = match.group(0)\n",
    "        match_start = match.start()\n",
    "        enclosing_div_start = input_html.rfind('<div', 0, match_start)\n",
    "        start_positions.append(enclosing_div_start)\n",
    "        \n",
    "    # return the starting position of the first incidence of \"PART I\" in the document\n",
    "    logging.info(f\"Filing will be processed from the following position onwards: {enclosing_div_start}. All preceding text will be ignored.\")\n",
    "    return start_positions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the html to readable text, cleaning up the document and stripping out unnecessary characters\n",
    "def extract_readable_text(input_html):\n",
    "    # Initialize BeautifulSoup with input HTML\n",
    "    soup = BeautifulSoup(input_html, 'html.parser')\n",
    "\n",
    "    # Remove all script, style, a, and img tags\n",
    "    for data in soup(['script', 'style', 'a', 'img', 'xbrl']):\n",
    "        data.decompose()\n",
    "\n",
    "    # Remove page numbers (div tags with centered text)\n",
    "    for div in soup.find_all('div', {'style': 'text-align:center;'}):\n",
    "        div.decompose()\n",
    "    \n",
    "    # Remove page numbers (p tags with centered text and containing digit)        \n",
    "    for p in soup.find_all('p'):\n",
    "        style = p.attrs.get('style', '')\n",
    "        if 'text-align:center;' in style and ('font-size:7.5pt;' in style or 'font-size:8pt;' in style):\n",
    "            # Your code here\n",
    "            p.decompose()\n",
    "\n",
    "    # Decide whether to process div or p tags based on their presence\n",
    "    p_tags = soup.find_all('p')\n",
    "    div_tags = soup.find_all('div')\n",
    "\n",
    "    if len(p_tags) > len(div_tags):\n",
    "        tags = p_tags\n",
    "    else:\n",
    "        tags = div_tags\n",
    "\n",
    "    # Initialize output text\n",
    "    output_text = ''\n",
    "\n",
    "    dollar_next = False\n",
    "\n",
    "    for tag in tags:\n",
    "        # Ignore tags that contain other same tags\n",
    "        if tag.find(tag.name):\n",
    "            continue\n",
    "\n",
    "        # Extract the first span tag in the tag\n",
    "        span = tag.find('span')\n",
    "\n",
    "        if span:\n",
    "            # Check the span for the style attribute and 'font-weight:bold'\n",
    "            style = span.attrs.get('style', '')\n",
    "            if 'font-weight:bold' in style:\n",
    "                text = span.text\n",
    "            else:\n",
    "                text = tag.text\n",
    "        else:\n",
    "            # If no span, use tag text\n",
    "            text = tag.text\n",
    "\n",
    "        # Replace '\\xa0' with a space character\n",
    "        text = text.replace('\\xa0', ' ')\n",
    "        \n",
    "        # Replace '•' '◦' with an empty string character\n",
    "        text = text.replace('•', '')\n",
    "        text = text.replace('◦', '')\n",
    "\n",
    "        # If this tag contains only '$', remember it to concatenate it with the next number\n",
    "        if text.strip() == '$':\n",
    "            dollar_next = True\n",
    "            continue\n",
    "\n",
    "        # If this tag contains only ')', append ')' to the previous text\n",
    "        if text.strip() == ')':\n",
    "            output_text = output_text.rstrip('\\n\\n') + ')\\n\\n'\n",
    "            continue\n",
    "\n",
    "        # If this tag contains only '%', append '%' to the previous text\n",
    "        if text.strip() == '%':\n",
    "            output_text = output_text.rstrip('\\n\\n') + '%\\n\\n'\n",
    "            continue\n",
    "        \n",
    "        # If this tag contains only ')%', append ')%' to the previous text\n",
    "        if text.strip() == ')%':\n",
    "            output_text = output_text.rstrip('\\n\\n') + ')%\\n\\n'\n",
    "            continue\n",
    "\n",
    "        if dollar_next:\n",
    "            text = '$' + text\n",
    "            dollar_next = False\n",
    "\n",
    "        # Strip leading and trailing spaces and add the cleaned text to output,\n",
    "        # followed by two newline characters only if text is not empty\n",
    "        text = text.strip()\n",
    "        if text:\n",
    "            output_text += text + '\\n\\n'\n",
    "\n",
    "    return output_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_content(url: str, headers: Dict[str, str]) -> str:\n",
    "    try:\n",
    "        response = requests.get(url, headers=headers)\n",
    "        response.raise_for_status()\n",
    "    except requests.RequestException as e:\n",
    "        logging.error(f\"Error occurred while retrieving content from {url}: {str(e)}\")\n",
    "        return \"\"\n",
    "        \n",
    "    return response.text\n",
    "\n",
    "def save_content(content: str, filename: str):\n",
    "    try:\n",
    "        with open(filename, 'w') as f:\n",
    "            f.write(content)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error occurred while saving content to {filename}: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_filings(\n",
    "        tickers_or_ciks: List[str], \n",
    "        filing_date_start: str, \n",
    "        filing_date_end: str, \n",
    "        email_as_user_agent: str, \n",
    "        form_type: str = None, \n",
    "        parse_xml: bool = True):\n",
    "    \n",
    "    headers = {\n",
    "        'User-Agent': email_as_user_agent\n",
    "    }\n",
    "    \n",
    "    logging.info(\n",
    "            \"Use of this API is subject to the SEC terms and conditions \"\n",
    "            \"governing the EDGAR database. You should conduct your own \"\n",
    "            \"review of the terms to make sure they are acceptable for your \"\n",
    "            \"use case before proceeding.\"\n",
    "    )\n",
    "    \n",
    "    cik_numbers = get_ciks(tickers_or_ciks)\n",
    "    filings = get_filings(cik_numbers, filing_date_start, filing_date_end, email_as_user_agent, form_type)\n",
    "\n",
    "    for cik_number, filing_data in filings.items():\n",
    "        accession_numbers = filing_data.get('accessionNumber', [])\n",
    "\n",
    "        for accession_number in accession_numbers:\n",
    "            url = f'https://www.sec.gov/Archives/edgar/data/{cik_number}/{accession_number}.txt'\n",
    "            content = retrieve_content(url, headers)\n",
    "\n",
    "            if not content:\n",
    "                continue\n",
    "\n",
    "            if parse_xml:        \n",
    "                # Get the starting char of \"PART I\" - the juicy part of the 10-K\n",
    "                start_character = extract_starting_position(content)\n",
    "                end_character = len(content)\n",
    "\n",
    "                # Clean up the text\n",
    "                readable_text = extract_readable_text(content[start_character:end_character])\n",
    "\n",
    "                # Save the cleaned up .txt file\n",
    "                filename = f'{cik_number}-{accession_number}-{filing_data[\"form\"][0]}-cleaned.txt'\n",
    "                save_content(readable_text, filename)\n",
    "                logging.info(f\"File saved as {filename}\\n\")\n",
    "            else:\n",
    "                filename = f'{cik_number}-{accession_number}-{filing_data[\"form\"][0]}-raw.txt'\n",
    "                save_content(content, filename)\n",
    "                logging.info(f\"File saved as {filename}\\n\")\n",
    "\n",
    "            time.sleep(REQUEST_DELAY_SECONDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Example usage. This will save the cleaned .txt files in your working folder. It should take 10-20 secs per filing.\n",
    "tickers_or_ciks = ['META', 'GOOG', 'AAPL', 'F']  # Example list of tickers or CIKs\n",
    "filing_date_start = '2019-05-01'  # Example start date\n",
    "filing_date_end = '2020-05-01'  # Example end date\n",
    "email_as_user_agent = 'nick@gmail.com' # Example user agent email\n",
    "form_type = '10-K'  # Example form type, only 10-K supported at this time\n",
    "parse_xml = True\n",
    "\n",
    "# cik_numbers = get_ciks(tickers_or_ciks)\n",
    "# filings = get_filings(cik_numbers, filing_date_start, filing_date_end, email_as_user_agent, form_type)\n",
    "download_filings(tickers_or_ciks, filing_date_start, filing_date_end, email_as_user_agent, form_type, parse_xml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
